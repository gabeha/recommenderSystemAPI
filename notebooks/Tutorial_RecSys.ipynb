{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48ed2a093f30ccdb",
   "metadata": {},
   "source": [
    "# Recommender System\n",
    "\n",
    "## Introduction\n",
    "The project is designed to improve the quality of education through the use of AI technologies. The primary objective is to demonstrate how AI can facilitate personalized education within the UCM college program. This will be achieved by assisting UCM students in developing the program with their own individual learning pathway and providing adaptive teaching materials to their individual needs. To subsequently develop a system in collaboration with educational institutions that meets societal needs and expectations, providing a responsive educational chain in the fast-paced world.\n",
    "\n",
    "This notebook provides an overview of the recommender system we have built for UCM. It includes key aspects to help you understand the system's functionality and workflow. We'll begin with how to call the system and then delve into the details of each component.\n",
    "\n",
    "Note: We are not Software Engineers, so the structure might not be the best. We are open to suggestions and improvements.\n",
    "\n",
    "Before starting, it's necessary to install the system's requirements. These are listed in the `requirements.txt` file. To install them, run the following command in your terminal:\n",
    "\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "\n",
    "Additionally, when you run the system for the first time, some transformer models will be downloaded. This will take some time in the first run. Moreover, we use MongoDB for the system. You need to have MongoDB installed on your system. You can download MongoDB from [here](https://www.mongodb.com/try/download/community).\n",
    "\n",
    "## Table of Contents:\n",
    "1. [Explainability](#Explainability)\n",
    "2. [Initialization and Recommender System Structure](#Initialization-and-Recommender-System-Structure)\n",
    "3. [Keyword Similarity](#Keyword-Similarity)\n",
    "4. [Bloom's Taxonomy](#Bloom's-Taxonomy)\n",
    "5. [Explanation with Generative LLM](#Explanation-with-Generative-LLM)\n",
    "6. [Planner (Integer Linear Programming)](#Planner-(Integer-Linear-Programming))\n",
    "7. [Warning System](#Warning-System)\n",
    "8. [Student Model](#Student-Model)\n",
    "9. [Reranker](#Reranker)\n",
    "10. [Knowledge Graph](#Knowledge-Graph)\n",
    "\n",
    "[GitHub](https://github.com/gabeha/recommenderSystemAPI); The project is large, so we have created a GitHub repository.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3543634fa4f0a3ef",
   "metadata": {},
   "source": [
    "## Explainability:\n",
    "\n",
    "This part is the most crucial, since our model will be used in a real-world environment with human interaction. Therefore, the explainability of models is an important aspect to be accepted by users and to comply with country regulations.\n",
    "\n",
    "**XAI requires:**\n",
    "\n",
    "- **Transparency** is given “if the processes that extract model parameters from training data and generate labels from testing data can be described and motivated by the approach designer”.\n",
    "\n",
    "- **Interpretability** describes the possibility to comprehend the ML model and to present the underlying basis for decision-making in a way that is understandable to humans.\n",
    "\n",
    "- **Explainability** is a concept that is recognized as important, but a joint definition is not yet available. It is suggested that explainability in ML can be considered as “the collection of features of the interpretable domain, that have contributed for a given example to produce a decision (e.g., classification or regression)”.\n",
    "\n",
    "sourse: DARPA\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/darpa.png\" alt=\"DARPA\" title=\"DARPA\"/>\n",
    "</p>\n",
    "\n",
    "Explanation are part of a social interaction:\n",
    "\n",
    "- Valid explanations should be sound, cogent, convincing and create trust.\n",
    "- The receiving party should be able to understand it.\n",
    "- Step by step transfer of knowledge\n",
    "\n",
    "Moreover explanations:\n",
    "\n",
    "- Must show the difference between possible outcomes of decision.\n",
    "- Should be relevant.\n",
    "- Can be incomplete. Emphasizing a few examples may be sufficient. No need to be overly complete.\n",
    "- Should be believable;\n",
    "\n",
    "**XAI in NLP** is becoming more important as Large Language models are used for a variety of mission-critical and high-impact applications in our society.\n",
    "\n",
    "Several overview papers have been written recently, investigating existing approaches for XAI in NLP. Here are two important ones:\n",
    "\n",
    "1) [Analysis Methods in Neural Language Processing: A Survey Yonatan Belinkov and James Glass](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00254/43503/Analysis-Methods-in-Neural-Language-Processing-A)\n",
    "\n",
    "2) [A Survey of the State of Explainable AI for Natural Language Processing Marina Danilevsky, Kun Qian, Ranit Aharonov, Yannis Katsis, Ban Kawas, Prithviraj Sen](https://arxiv.org/abs/2010.00711)\n",
    "\n",
    "Relevant questions to ask are: what linguistic knowledge is captured by neural networks, why they make certain predictions, if they are robust, interpreting the way they represent language and how they fail.\n",
    "\n",
    "But how do you explain complex mathematics and machine learning algorithms to individuals that are not familiar with those concepts, let alone, really understand these?\n",
    "\n",
    "In the following paper, these problems are discussed:\n",
    "\n",
    "1) [The Mythos of Model Interpretability by Zachary C. Lipton](https://arxiv.org/abs/1606.03490)\n",
    "\n",
    "2) [DARPA program onExplainable Artificial Intelligence (XAI) by Dr. Matt Turek](https://www.darpa.mil/program/explainable-artificial-intelligence)\n",
    "\n",
    "Both are important to understand psychological aspects of \"explainability\" and \"understanding\" and different approaches taking in similar situations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42da83684d684bbc",
   "metadata": {},
   "source": [
    "## Initialization and Recommender System Structure:\n",
    "\n",
    "General Structure of the Recommender System:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/rs.png\" alt=\"Recommender System\" title=\"Recommender System\"/>\n",
    "</p>\n",
    "\n",
    "The file `instances/recommender_instances.py` contains the initialization of the recommender system. Here, we import all the components that are required for the system. The components are initialized once and then used throughout the system. The system's structure is kept as modular as possible. This modularity aids in the easy integration of new components into the system and in its maintenance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95a857797e8e0f5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T19:35:36.770319500Z",
     "start_time": "2023-12-24T19:35:36.754320300Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# First we change the directory to the root directory of the project. Where we usually run the app.py file.\n",
    "os.chdir(os.getcwd().replace(\"\\\\notebooks\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a0f319d68bc8d7",
   "metadata": {},
   "source": [
    "Recommender System Moduls:\n",
    "```\n",
    "# Keyword Based\n",
    "keyword_based = KeywordBased(model_name=\"all-MiniLM-L12-v2\",\n",
    "                             seed_help=True,\n",
    "                             domain_adapt=True,\n",
    "                             zero_adapt=True,\n",
    "                             domain_type='title',\n",
    "                             seed_type='title',\n",
    "                             zero_type='title',\n",
    "                             score_alg='sum',\n",
    "                             distance='cos',\n",
    "                             backend='keyBert',\n",
    "                             scaler=\"MinMax\",\n",
    "                             sent_splitter=False,\n",
    "                             precomputed_course=True)\n",
    "\n",
    "# Content Based\n",
    "content_based = ContentBased(\n",
    "                             model_name='BAAI/bge-large-en-v1.5',\n",
    "                             distance='cos',\n",
    "                             score_alg='sum',\n",
    "                             scaler='None')\n",
    "\n",
    "# Explanation model\n",
    "explanation = LLM(\n",
    "    token=\"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\"\n",
    ")\n",
    "\n",
    "# Bloom Based\n",
    "bloom_based = BloomBased(score_alg='sum',\n",
    "                         scaler='None')\n",
    "\n",
    "# Warning model\n",
    "warning_model = WarningModel()\n",
    "\n",
    "# Planner model\n",
    "planner = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "422acd1016852f38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T15:41:58.517396Z",
     "start_time": "2023-12-24T15:41:50.506181200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CourseBasedRecSys config: \n",
      "model_name: all-MiniLM-L12-v2\n",
      "seed_help: True\n",
      "domain_adapt: True\n",
      "zero_adapt: True\n",
      "seed_type: title\n",
      "domain_type: title\n",
      "zero_type: title\n",
      "adaptive_thr: 0.0\n",
      "minimal_similarity_zeroshot: 0.8\n",
      "score_alg: sum\n",
      "distance: cos\n",
      "backend: keyBert\n",
      "scaler: MinMax\n",
      "sent_splitter: False\n",
      "precomputed_course: True\n",
      "\n",
      "Loading warning models: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "754a375ce4e84057b16090eabdd6b6f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning model loaded successfully!\n",
      "\n",
      "RecSys settings: \n",
      "Validate_input: True \n",
      "Keyword_based: <rec_sys_uni.rec_systems.keyword_based_sys.keyword_based.KeywordBased object at 0x000001C8591AAE00> \n",
      "Bloom_based: <rec_sys_uni.rec_systems.bloom_based_sys.bloom_based.BloomBased object at 0x000001C823036080> \n",
      "Explanation: <rec_sys_uni.rec_systems.llm_explanation.LLM.LLM object at 0x000001C822B23550> \n",
      "Warning_model: <rec_sys_uni.rec_systems.warning_model.warning_model.WarningModel object at 0x000001C8230360E0> \n",
      "Planner: None \n",
      "Top_n: 7 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<rec_sys_uni._rec_sys_helpers.StudentNode at 0x1c85943a3b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is example code for the initialization of the system.\n",
    "from instances.recommender_instances import rs\n",
    "\n",
    "# Now with the rs object we can call the components that we need.\n",
    "input = { # Example of the input that we need for the system. Also, this is the input that we get from the front-end.\n",
    "    \"keywords\": {\n",
    "        \"physics\": 1.0,\n",
    "        \"maths\": 1.0,\n",
    "        \"statistics\": 1.0,\n",
    "        \"ai\": 1.0,\n",
    "        \"computer science\": 1.0,\n",
    "        \"chem\": 1.0,\n",
    "    },\n",
    "    \"blooms\": {'create': 0.0,\n",
    "               'understand': 0.0,\n",
    "               'apply': 0.0,\n",
    "               'analyze': 0.0,\n",
    "               'evaluate': 0.0,\n",
    "               'remember': 0.0}\n",
    "}\n",
    "student_node = rs.get_recommendation(input) # This will return the recommendations for the input as a Student Node object.\n",
    "student_node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4a928bb13d328a",
   "metadata": {},
   "source": [
    "The `StudentNode` object contains all information about the student. It is a class that we have created to keep the information about the student. It contains the following attributes:\n",
    "\n",
    "1. **results**: contains results from the recommender system components.\n",
    "2. **student_input**: contains keywords and Bloom's taxonomy of the student input.\n",
    "3. **course_data**: currently we are using system course data, but later on, the front-end should send the course list for which we need to compute recommendation.\n",
    "4. **student_data**: contains all courses which the student took.\n",
    "5. **id**: unique Student ID, which was assigned by MongoDB.\n",
    "\n",
    "The `StudentNode` object is saved in the MongoDB database. We use the `id` attribute as the unique identifier for the student. We use the `id` attribute to retrieve the `StudentNode` object from the database. It helps call other components of the system independently without sending unnecessary information to the front-end.\n",
    "\n",
    "Let's take a look at each component of the student node at detail.\n",
    "\n",
    "### 1. Results:\n",
    "`results`: A dictionary of recommended courses. For example:\n",
    "\n",
    "- `recommended_courses`: A dictionary where each key is a `course_id` (String) and the value is another dictionary with the following keys:\n",
    "  - `score`: A float representing the total score of each model.\n",
    "  - `period`: A list of integers or a list of lists of integers. For example, `[1, 4]` or `[[1, 2, 3], [4, 5, 6]]` or `[[1,2]]`.\n",
    "  - `warning`: A boolean value.\n",
    "  - `warning_recommendation`: A list of warning recommendations.\n",
    "  - `keywords`: A dictionary of scores for each keyword (only present after applying the CourseBased model).\n",
    "  - `blooms`: A dictionary of scores for each bloom (only present after applying the BloomBased model).\n",
    "\n",
    "- `sorted_recommended_courses`: A list of courses, which are included to the structured recommendation, where each course is a dictionary with the following keys:\n",
    "  - `course_code`: A string.\n",
    "  - `course_name`: A string.\n",
    "  - `warning`: A boolean value.\n",
    "  - `warning_recommendation`: A list of warning recommendations.\n",
    "  - `keywords`: A dictionary where each key is a keyword (String) and the value is a weight (float).\n",
    "  - `blooms`: A dictionary where each key is a bloom (String) and the value is a weight (float).\n",
    "  - `score`: A float.\n",
    "\n",
    "- `structured_recommendation`: A dictionary with the following structure:\n",
    "  - `semester_1`: A dictionary with the following keys:\n",
    "    - `period_1`: A list of `course_id` (String), the length of the list is less than or equal to `top_n`.\n",
    "    - `period_2`: A list of `course_id` (String), the length of the list is less than or equal to `top_n`.\n",
    "  - `semester_2`: A dictionary with the following keys:\n",
    "    - `period_4`: A list of `course_id` (String), the length of the list is less than or equal to `top_n`.\n",
    "    - `period_5`: A list of `course_id` (String), the length of the list is less than or equal to `top_n`.\n",
    "\n",
    "Note: If you apply the `sort_by_periods` function, you will have `structured_recommendation` and `sorted_recommended_courses` keys in the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae4cf5bdd3ad52f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T15:42:16.355436700Z",
     "start_time": "2023-12-24T15:42:16.353436600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['recommended_courses', 'sorted_recommended_courses', 'structured_recommendation'])\n",
      "Number of courses: 149\n",
      "dict_keys(['course_code', 'course_name'])\n",
      "dict_keys(['semester_1', 'semester_2'])\n",
      "dict_keys(['period_1', 'period_2'])\n",
      "dict_keys(['course_code', 'course_name'])\n"
     ]
    }
   ],
   "source": [
    "print(student_node.results.keys())\n",
    "print(\"Number of courses: \" + str(len(student_node.results[\"recommended_courses\"].keys())))\n",
    "print(student_node.results[\"sorted_recommended_courses\"][0].keys())\n",
    "print(student_node.results[\"structured_recommendation\"].keys())\n",
    "print(student_node.results[\"structured_recommendation\"][\"semester_1\"].keys())\n",
    "print(student_node.results[\"structured_recommendation\"][\"semester_1\"][\"period_1\"][0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca1aa25a0c88f26",
   "metadata": {},
   "source": [
    "### 2. Student Input:\n",
    "`student_input`: A dictionary that contains the student's input. For example:\n",
    "\n",
    "- `keywords`: A dictionary where each key is a keyword (String) and the value is a weight (float).\n",
    "- `blooms`: A dictionary where each key is a bloom (String) and the value is a weight (float).\n",
    "\n",
    "Example of `student_input`:\n",
    "\n",
    "```python\n",
    "student_input = {\n",
    "    \"keywords\": {\n",
    "        \"python\": 0.5,\n",
    "        \"data science\": 0.2\n",
    "    },\n",
    "    \"blooms\": {\n",
    "        \"create\": 0.5,\n",
    "        \"understand\": 0.75,\n",
    "        \"apply\": 0.25,\n",
    "        \"analyze\": 0.5,\n",
    "        \"evaluate\": 0.0,\n",
    "        \"remember\": 1.0\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### 3. Course Data:\n",
    "`course_data`: A dictionary where each key is a `course_id` (String) and the value is another dictionary with the following keys:\n",
    "\n",
    "- `course_name`: A string representing the name of the course. For example, \"Philosophy of Science\".\n",
    "- `period`: A list of integers or a list of lists of integers. For example, `[1, 4]` or `[[1, 2, 3], [4, 5, 6]]` or `[[1,2]]`.\n",
    "- `level`: An integer representing the level of the course. For example, 1, 2, or 3.\n",
    "- `prerequisites`: A list of course IDs (Strings) that are prerequisites for the course. For example, `[\"COR1001\", \"COR1002\"]`.\n",
    "- `description`: A string providing a description of the course. For example, \"This course is about ...\".\n",
    "- `ilos`: A list of strings representing the Intended Learning Outcomes (ILOs) of the course. For example, [\"Be able to apply the scientific method to a given problem\", \"Be able to explain the difference between science and pseudoscience\"].\n",
    "\n",
    "Example of `course_data`:\n",
    "\n",
    "```python\n",
    "course_data = {\n",
    "    \"COR1001\": {\n",
    "        \"course_name\": \"Philosophy of Science\",\n",
    "        \"period\": [1, 4],\n",
    "        \"level\": 1,\n",
    "        \"prerequisites\": [\"COR1002\"],\n",
    "        \"description\": \"This course is about ...\",\n",
    "        \"ilos\": [\n",
    "            \"Be able to apply the scientific method to a given problem\",\n",
    "            \"Be able to explain the difference between science and pseudoscience\"\n",
    "        ]\n",
    "    },\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "Note: We are using the `course_data` from the system. However, later on, the front-end should send the course list for which we need to compute recommendation.\n",
    "\n",
    "### 4. Student Data:\n",
    "`student_data`: A dictionary that contains the student's data. For example:\n",
    "\n",
    "- `courses_taken`: A dictionary where each key is a `course_id` (String) and the value is another dictionary with the following keys:\n",
    "  - `passed`: A boolean value indicating whether the student passed the course.\n",
    "  - `grade`: A float representing the grade the student received in the course.\n",
    "  - `period`: An integer or a list of two integers representing the period(s) when the course was taken.\n",
    "  - `year`: An integer representing the year when the course was taken.\n",
    "\n",
    "Example of `student_data`:\n",
    "\n",
    "```python\n",
    "student_data = {\n",
    "    \"courses_taken\": {\n",
    "        \"COR1001\": {\n",
    "            \"passed\": True,\n",
    "            \"grade\": 3.5,\n",
    "            \"period\": [1, 2],\n",
    "            \"year\": 2022\n",
    "        },\n",
    "        ...\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Note: We are using the `student_data` from the system. However, later on, the front-end should send the student's data along with student ID, which was provided by Maastricht University.\n",
    "\n",
    "### Usage of MongoDB:\n",
    "\n",
    "We use MongoDB to store the `StudentNode` object. We use the `id` attribute to retrieve the `StudentNode` object from the database. We show an example of how to use MongoDB to retrieve the `StudentNode` object in the json format.\n",
    "\n",
    "Local server: `mongodb://localhost:27017/`\n",
    "\n",
    "In the RecSys a.k.a `rs` object, we have a `db` attribute which is the database that we are using. We use a `RecSys` database. We store the `StudentNode` object in the `student_results` collection. We use the `id` attribute as the unique identifier for the student's session. \n",
    "\n",
    "Note: Every time, when student input is sent to the system, we create a new `StudentNode` object and store it in the database.\n",
    "\n",
    "Example: How to use the `id` attribute to retrieve the `StudentNode` object as json from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8861834943f7790",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T15:42:56.932707900Z",
     "start_time": "2023-12-24T15:42:56.920201700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "658851466c10295417bafda1\n"
     ]
    }
   ],
   "source": [
    "# This is an example of id that we get from MongoDB.\n",
    "print(student_node.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ccb58991b5d3f74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T15:43:01.580950700Z",
     "start_time": "2023-12-24T15:43:01.564950700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['_id', 'student_id', 'student_input', 'course_data', 'student_data', 'results', 'time'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bson.objectid import ObjectId\n",
    "# First we need to get the database from the rs object.\n",
    "db = rs.db\n",
    "\n",
    "# Then we get the collection from the database.\n",
    "collection = db[\"student_results\"]\n",
    "\n",
    "# We can retrieve the student node object in the json format from the collection by their id.\n",
    "student_info = collection.find({\"_id\": ObjectId(str(student_node.id))})[0]\n",
    "student_info.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a411a0edf20b8ceb",
   "metadata": {},
   "source": [
    "Let's briefly explain the parameters and functions of the Recommender System:\n",
    "```\n",
    "def __init__(self,\n",
    "                 keyword_based: KeywordBased = None,\n",
    "                 bloom_based: BloomBased = None,\n",
    "                 explanation: LLM = None,\n",
    "                 warning_model: WarningModel = None,\n",
    "                 planner: UCMPlanner = None,\n",
    "                 validate_input: bool = True,\n",
    "                 top_n: int = 7,\n",
    "                 ):\n",
    "        self.validate_input = validate_input\n",
    "        self.keyword_based = keyword_based\n",
    "        self.bloom_based = bloom_based\n",
    "        self.explanation = explanation\n",
    "        self.warning_model = warning_model\n",
    "        self.planner = planner\n",
    "        self.top_n = top_n\n",
    "        self.db = pymongo.MongoClient(\"mongodb://localhost:27017/\")[\"RecSys\"]\n",
    "```\n",
    "- `keyword_based`: KeywordBased model.\n",
    "- `bloom_based`: BloomBased model.\n",
    "- `explanation`: LLM model.\n",
    "- `warning_model`: WarningModel model.\n",
    "- `planner`: UCMPlanner model.\n",
    "- `validate_input`: Boolean value. If True, the input from front-end will be validated.\n",
    "- `top_n`: Integer value. The number of courses that will be recommended in the period.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af354ec35bec1945",
   "metadata": {},
   "source": [
    "```\n",
    "def validate_system_input(self,\n",
    "                              student_input,\n",
    "                              course_data,\n",
    "                              student_data,\n",
    "                              system_course_data,\n",
    "                              system_student_data):\n",
    "        \"\"\"\n",
    "        function: validate_system_input\n",
    "        description: validate the format of the input data\n",
    "        \"\"\"\n",
    "\n",
    "        check_student_input(student_input)\n",
    "\n",
    "        if system_student_data:\n",
    "            student_data = get_student_data()\n",
    "\n",
    "        if system_course_data:\n",
    "            exception_courses = []\n",
    "            if student_data:\n",
    "                exception_courses = list(student_data['courses_taken'].keys())\n",
    "            course_data = get_course_data(except_courses=exception_courses)\n",
    "\n",
    "        check_course_data(course_data)\n",
    "\n",
    "        if student_data is not None:\n",
    "            check_student_data(student_data)\n",
    "\n",
    "        return student_input, course_data, student_data\n",
    "```\n",
    "- function `validate_system_input`: Validate the format of the input data. If the input data is not in the correct format, it will raise an error. If the input data is in the correct format, it will return the input data. Additionally, we include system `course_data` and `student_data` in the input data if they are not provided by the front-end, but in later stage it should be provided by the front-end. All rules for the input data are defined in the `rec_sys_uni/error_checker/errors.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a33493c95e77f8fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T15:43:23.622624500Z",
     "start_time": "2023-12-24T15:43:23.618622600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "student_input does not have keywords\n"
     ]
    }
   ],
   "source": [
    "input = { # Lets make a mistake in the input data.\n",
    "    \"keyword\": { # Instead of keywords, we wrote keyword.\n",
    "        \"physics\": 1.0,\n",
    "        \"maths\": 1.0,\n",
    "        \"statistics\": 1.0,\n",
    "        \"ai\": 1.0,\n",
    "        \"computer science\": 1.0,\n",
    "        \"chem\": 1.0,\n",
    "    },\n",
    "    \"blooms\": {'create': 0.0,\n",
    "               'understand': 0.0,\n",
    "               'apply': 0.0,\n",
    "               'analyze': 0.0,\n",
    "               'evaluate': 0.0,\n",
    "               'remember': 0.0}\n",
    "}\n",
    "try:\n",
    "    student_node = rs.get_recommendation(input)\n",
    "except Exception as e:\n",
    "    print(e) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278561a135eb172",
   "metadata": {},
   "source": [
    "`get_recommendation` function is the main function of the system. It calls all the components of the system, which calculate recommendation and returns the `StudentNode` object. It takes the following parameters:\n",
    "```\n",
    "def get_recommendation(self,\n",
    "                           student_intput,\n",
    "                           course_data=None,\n",
    "                           student_data=None,\n",
    "                           system_course_data=True,\n",
    "                           system_student_data=False,\n",
    "                           ):\n",
    "        \"\"\"\n",
    "        function: get_recommendation\n",
    "        description: get Student Node object\n",
    "        \"\"\"\n",
    "\n",
    "        if self.validate_input:\n",
    "            student_input, course_data, student_data = self.validate_system_input(student_intput,\n",
    "                                                                                  course_data,\n",
    "                                                                                  student_data,\n",
    "                                                                                  system_course_data,\n",
    "                                                                                  system_student_data)\n",
    "\n",
    "        # Make results template\n",
    "        results = make_results_template(course_data)\n",
    "\n",
    "        # Create StudentNode object\n",
    "        student_info = StudentNode(results, student_intput, course_data, student_data)\n",
    "\n",
    "        # Compute recommendation and store in student_info\n",
    "        compute_recommendation(self, student_info)\n",
    "\n",
    "        # Compute warnings and store in student_info\n",
    "        if self.warning_model:\n",
    "            compute_warnings(self, student_info)\n",
    "\n",
    "        # Sort by periods\n",
    "        sort_by_periods(self, student_info, self.top_n, include_keywords=True, include_score=True,\n",
    "                        include_blooms=False)\n",
    "\n",
    "        # Save student_info to database\n",
    "        now = datetime.now()\n",
    "\n",
    "        collection = self.db[\"student_results\"]\n",
    "        input_dict = {\n",
    "            \"student_id\": \"123\",\n",
    "            \"student_input\": student_info.student_input,\n",
    "            \"course_data\": student_info.course_data,\n",
    "            \"student_data\": student_info.student_data,\n",
    "            \"results\": student_info.results,\n",
    "            \"time\": now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "        }\n",
    "        object_db = collection.insert_one(input_dict)\n",
    "        student_info.set_id(object_db.inserted_id)\n",
    "\n",
    "        # Return StudentNode object\n",
    "        return student_info\n",
    "```\n",
    "- `student_input`: A dictionary that contains the student's input.\n",
    "- `course_data`: A dictionary that contains the course data.\n",
    "- `student_data`: A dictionary that contains the student's data.\n",
    "- `system_course_data`: Boolean value. If True, the system course data will be used.\n",
    "- `system_student_data`: Boolean value. If True, the system student data will be used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49a97237739ecb4",
   "metadata": {},
   "source": [
    "`generate_explanation` function is used to generate the explanation for the recommendation. It takes the following parameters:\n",
    "- `student_id`: A string representing the student's ID from MongoDB.\n",
    "- `course_code`: A string representing the course's code.\n",
    "\n",
    "`make_timeline` function is used to generate the timeline for the recommendation. It takes the following parameters:\n",
    "- `student_id`: A string representing the student's ID from MongoDB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d888e037d625513",
   "metadata": {},
   "source": [
    "## Keyword Similarity:\n",
    "\n",
    "Our Keyword Similarity model is designed to assess the degree of similarity between a student's specified keywords and the content within course descriptions. To accomplish this, various models can be employed, particularly those available through the HuggingFace library ([leaderboard](https://huggingface.co/spaces/mteb/leaderboard)), which is renowned for its diverse range of pre-trained models and natural language processing tools. These models from HuggingFace are adept at understanding and processing language, thus making them ideal for accurately gauging the relevance between keywords and course content.\n",
    "\n",
    "In addition to the options available in HuggingFace, there is also the possibility of utilizing an [Intel-based model](https://huggingface.co/Intel). This approach can be particularly advantageous when working with Intel server CPUs, as the model is optimized to efficiently compute embeddings on these processors. The use of an Intel model could potentially offer improved performance in terms of speed and efficiency, especially in environments that are already using Intel hardware.\n",
    "\n",
    "The choice between these models should be guided by several factors, including the specific requirements of the task, the available computational resources, and the desired balance between accuracy and efficiency. HuggingFace models might offer a wider range of capabilities and potentially higher accuracy due to their bigger model and extensive training on diverse datasets. On the other hand, an Intel-optimized model might provide faster processing times and better integration with existing Intel server infrastructures, making it a practical choice for environments prioritizing efficiency and hardware compatibility.\n",
    "\n",
    "Ultimately, the decision on which model to use for computing keyword-course description similarity should be based on a thorough evaluation of the project's needs and the computational environment.\n",
    "\n",
    "Let's take a look at the `KeywordBased` class initialization:\n",
    "```\n",
    "# Keyword Based\n",
    "keyword_based = KeywordBased(model_name=\"all-MiniLM-L12-v2\",\n",
    "                             seed_help=True,\n",
    "                             domain_adapt=True,\n",
    "                             zero_adapt=True,\n",
    "                             domain_type='title',\n",
    "                             seed_type='title',\n",
    "                             zero_type='title',\n",
    "                             score_alg='sum',\n",
    "                             distance='cos',\n",
    "                             backend='keyBert',\n",
    "                             scaler=True,\n",
    "                             sent_splitter=False,\n",
    "                             precomputed_course=True)\n",
    "```\n",
    "\n",
    "- `model_name`: model name of the sentence transformer, specifically from the HuggingFace library.\n",
    "- `seed_help`: apply seed help, mix course description with seed words embeddings\n",
    "- `domain_adapt`: apply domain adaptation, add additional attention layer to the model\n",
    "- `zero_adapt`: apply zero-shot adaptation, more advance seed filtering\n",
    "- `seed_type`: type of the seed help either 'title' or 'domains', we use 'title' for now, since 'domains' did not show good performance\n",
    "- `domain_type`: type of the domain adaptation either 'title' or 'domains', we use 'title' for now, since 'domains' did not show good performance\n",
    "- `zero_type`: type of the zero-shot adaptation either 'title' or 'domains', we use 'title' for now, since 'domains' did not show good performance\n",
    "- `adaptive_thr`: adaptive threshold for the zero-shot adaptation\n",
    "- `minimal_similarity_zeroshot`: minimal similarity between a candidate and a domain word for the zero-shot adaptation\n",
    "- `score_alg`: score algorithm either 'sum' or 'rrf', currently we use 'sum', but Relevance Ranking Fusion (RRF) can be used, but with another implementation of keyword similarity\n",
    "- `distance`: distance metric either 'cos' or 'dot', for some models is required to use 'dot' instead of 'cos'.\n",
    "- `backend`: backend either 'keyBert' or 'Intel'. 'Intel' is used to compute the embeddings on Intel server CPU, but it provides only advance models like BGE, which can be too large for our system.\n",
    "- `scaler`: apply min-max scaler to the keywords weights.\n",
    "- `sent_splitter`: split the course description into sentences. It is implemented to use Linear Programing to calculate scores, but it still needs to be implemented.\n",
    "- `precomputed_course`: use precomputed course embeddings or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ec3738b748087d",
   "metadata": {},
   "source": [
    "If you are planning to use domain adaptation, zero-shot adaptation, or both, it is necessary to precompute the embeddings and the attention layer in advance. You can use the following code to precompute the embeddings and attention layer:\n",
    "\n",
    "```\n",
    "def calculate_zero_shot_adaptation(course_data, model_name, attention,\n",
    "                                   adaptive_thr: float = 0.15,\n",
    "                                   minimal_similarity_zeroshot: float = 0.8):\n",
    "```\n",
    "- `course_data`: A dictionary or list containing the course code. Please follow the template that we discussed earlier.\n",
    "- `model_name`: The name of the sentence transformer model, specifically from the HuggingFace library, for which you want to precompute the embedding for zero-shot adaptation.\n",
    "- `attention`: The specific words to which you want to apply attention in order to bring the embedding closer to the course domain within the embedding space.\n",
    "\n",
    "```\n",
    "def calculate_few_shot_adaptation(course_data, model_name, attention,\n",
    "                                  lr=1e-4, epochs=100,  # Training Parameters\n",
    "                                  start_index=0,\n",
    "                                  include_description=True,\n",
    "                                  include_title=False,\n",
    "                                  include_ilos=False, ):\n",
    "```\n",
    "- `course_data`: A dictionary containing course descriptions, Intended Learning Outcomes (ILOs), and the course code. Please follow the template that we discussed earlier.\n",
    "- `model_name`: The name of the sentence transformer model, specifically from the HuggingFace library, for which you want to precompute the embedding for few-shot adaptation.\n",
    "- `attention`: The specific words to which you want to apply attention in order to bring the embedding closer to the course domain within the embedding space.\n",
    "- `lr`: Learning rate.\n",
    "- `epochs`: Number of epochs.\n",
    "\n",
    "```\n",
    "def calculate_precomputed_courses(course_data, model_name,\n",
    "                                  include_description=True,\n",
    "                                  include_title=False,\n",
    "                                  include_ilos=False, ):\n",
    "```\n",
    "- `course_data`: A dictionary containing course descriptions, Intended Learning Outcomes (ILOs), and the course code. Please follow the template that we discussed earlier.\n",
    "- `model_name`: The name of the sentence transformer model, specifically from the HuggingFace library, for which you want to precompute the embedding for the course.\n",
    "- `include_description`: Boolean value. If True, the course description will be included in the embedding.\n",
    "- `include_title`: Boolean value. If True, the course title will be included in the embedding.\n",
    "- `include_ilos`: Boolean value. If True, the course ILOs will be included in the embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7ca6c6042979bc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T16:41:15.404805400Z",
     "start_time": "2023-12-24T16:41:15.221913900Z"
    }
   },
   "outputs": [],
   "source": [
    "from rec_sys_uni.datasets.datasets import calculate_precomputed_courses, calculate_few_shot_adaptation, calculate_zero_shot_adaptation, get_course_data\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bdb34088a12aafc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T16:41:15.799514800Z",
     "start_time": "2023-12-24T16:41:15.633476600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['COR1002', 'COR1003', 'COR1004', 'COR1006', 'HUM1003', 'HUM1007', 'HUM1010', 'HUM1011', 'HUM1012', 'HUM1013', 'HUM1016', 'HUM2003', 'HUM2005', 'HUM2007', 'HUM2008', 'HUM2013', 'HUM2016', 'HUM2018', 'HUM2021', 'HUM2022', 'HUM2030', 'HUM2031', 'HUM2046', 'HUM2047', 'HUM2051', 'HUM2054', 'HUM2056', 'HUM2057', 'HUM2058', 'HUM2059', 'HUM2060', 'HUM3014', 'HUM3019', 'HUM3029', 'HUM3034', 'HUM3036', 'HUM3040', 'HUM3042', 'HUM3043', 'HUM3044', 'HUM3045', 'HUM3049', 'HUM3050', 'HUM3051', 'HUM3052', 'HUM3053', 'SCI1004', 'SCI1005', 'SCI1009', 'SCI1010', 'SCI1016', 'SCI2002', 'SCI2009', 'SCI2010', 'SCI2011', 'SCI2017', 'SCI2018', 'SCI2019', 'SCI2022', 'SCI2031', 'SCI2033', 'SCI2034', 'SCI2035', 'SCI2036', 'SCI2037', 'SCI2039', 'SCI2040', 'SCI2041', 'SCI2042', 'SCI2043', 'SCI2044', 'SCI3003', 'SCI3005', 'SCI3006', 'SCI3007', 'SCI3046', 'SCI3049', 'SCI3050', 'SCI3051', 'SCI3052', 'SSC1005', 'SSC1007', 'SSC1025', 'SSC1027', 'SSC1029', 'SSC1030', 'SSC2002', 'SSC2004', 'SSC2006', 'SSC2007', 'SSC2008', 'SSC2009', 'SSC2010', 'SSC2011', 'SSC2018', 'SSC2019', 'SSC2020', 'SSC2022', 'SSC2024', 'SSC2025', 'SSC2027', 'SSC2028', 'SSC2029', 'SSC2037', 'SSC2039', 'SSC2043', 'SSC2046', 'SSC2048', 'SSC2050', 'SSC2053', 'SSC2055', 'SSC2060', 'SSC2061', 'SSC2062', 'SSC2063', 'SSC2064', 'SSC2065', 'SSC2070', 'SSC2071', 'SSC3002', 'SSC3003', 'SSC3006', 'SSC3008', 'SSC3009', 'SSC3011', 'SSC3012', 'SSC3013', 'SSC3017', 'SSC3018', 'SSC3019', 'SSC3023', 'SSC3030', 'SSC3032', 'SSC3033', 'SSC3034', 'SSC3036', 'SSC3038', 'SSC3040', 'SSC3041', 'SSC3047', 'SSC3049', 'SSC3051', 'SSC3052', 'SSC3054', 'SSC3055', 'SSC3056', 'SSC3059', 'SSC3060', 'SSC3061'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course_data = get_course_data()\n",
    "course_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39e79c383a189aef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T21:47:59.525494100Z",
     "start_time": "2023-12-13T21:47:59.380514900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['course_name', 'period', 'level', 'prerequisites', 'description', 'ilos'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course_data['COR1002'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592c3fff26963567",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_precomputed_courses(course_data, \"all-MiniLM-L12-v2\"\n",
    "                              , include_description=True\n",
    "                              , include_title=True\n",
    "                              , include_ilos=True\n",
    "                              , use_cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faf70cc64ea5fcdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T15:01:52.763016400Z",
     "start_time": "2023-12-12T15:01:52.627994Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(course_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961227cfb8f97c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = {}\n",
    "for i in course_data.keys():\n",
    "    attention[i] = course_data[i]['course_name']\n",
    "calculate_few_shot_adaptation(course_data, \"all-MiniLM-L12-v2\", attention=attention, use_cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbd4a700520cbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_zero_shot_adaptation(course_data, \"all-MiniLM-L12-v2\", attention=attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af350af8587dda92",
   "metadata": {},
   "source": [
    "You can find additional information about attention-based keyword similarity [here](https://arxiv.org/pdf/2211.07499.pdf). \n",
    "\n",
    "If you follow all the steps outlined above, you will be able to use the new model to compute the similarity between the student's keywords and the course descriptions. Note that all embeddings and layers will be downloaded automatically; just remember to change the model during the initialization of KeywordBased\n",
    "\n",
    "General structure of Keyword-based model:\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/kb.png\" alt=\"Keyword-based model\" title=\"Keyword-based model\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4571b19dc4376a64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T16:01:40.600584400Z",
     "start_time": "2023-12-24T16:01:38.294387600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'physics': 0.317,\n",
       " 'maths': 0.2752,\n",
       " 'statistics': 0.2255,\n",
       " 'ai': 0.1399,\n",
       " 'computer science': 0.2675,\n",
       " 'chem': 0.17}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rec_sys_uni.rec_systems.keyword_based_sys.keyword_based import KeywordBased\n",
    "keyword_based = KeywordBased(model_name=\"all-MiniLM-L12-v2\",\n",
    "                             seed_help=True,\n",
    "                             domain_adapt=True,\n",
    "                             zero_adapt=True,\n",
    "                             domain_type='title',\n",
    "                             seed_type='title',\n",
    "                             zero_type='title',\n",
    "                             score_alg='sum',\n",
    "                             distance='cos',\n",
    "                             backend='keyBert',\n",
    "                             scaler=\"MinMax\",\n",
    "                             sent_splitter=False,\n",
    "                             precomputed_course=True)\n",
    "\n",
    "keywords_output = keyword_based.recommend(student_node.course_data,\n",
    "                                             student_node.student_input['keywords'])\n",
    "keywords_output['COR1002']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884ac25be54578d",
   "metadata": {},
   "source": [
    "#### Future Improvements of Keyword Similarity:\n",
    "- Add evaluation metrics to compare performance of different models.\n",
    "- Add Explanation of the keyword similarity based on attention layers to get sentences and words that are similar to the keywords using exBert, LIME or other explanation techniques.\n",
    "\n",
    "ExBERT is another model explaining the innerworkings of Transformers developed by MIT-IBM AI Labs and Harvard NLP Group.\n",
    "\n",
    "ExBERT does not only provide insight into attention mechanisms, but also combines them with the contextual word embeddings to provide better insight in these important relations as well. According to the authors: \"While static analyses of these models lead to targeted insights, interactive tools are more dynamic and can help humans better gain an intuition for the model internal reasoning process. We present EXBERT, an interactive tool named after the popular BERT language model, that provides insights into the meaning of the contextual representations by matching a human-specified input to similar contexts in a large annotated dataset. By aggregating the annotations of the matching similar contexts, EXBERT helps intuitively explain what each attention-head has learned.\"\n",
    "\n",
    "The paper can be found here: [paper](https://arxiv.org/pdf/1910.05276.pdf) and the source code here: [github](https://github.com/bhoov/exbert)\n",
    "\n",
    "Reference: EXBERT: A Visual Analysis Tool to Explore Learned Representations in Transformers Models. Hoover et al. 2019.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/ExBERT.png\" alt=\"ExBERT\" title=\"ExBERT\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e69af0af2677299",
   "metadata": {},
   "source": [
    "## Bloom's Taxonomy:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e929c64c3b6a23e",
   "metadata": {},
   "source": [
    "[Bloom's taxonomy](https://en.wikipedia.org/wiki/Bloom%27s_taxonomy) describes a [multilabel classification](https://en.wikipedia.org/wiki/Multi-label_classification) problem, where an intended learning objective (ILO) is labeled according to some domain criterion. For our use case, we are specifically interested in the cognitive domain, where an ILO may be assigned to either of six categories:\n",
    "\n",
    "- create\n",
    "  \n",
    "- understand\n",
    "  \n",
    "- apply\n",
    "  \n",
    "- analyze\n",
    "  \n",
    "- evaluate\n",
    "  \n",
    "- remember\n",
    "  \n",
    "These categories reflect cognitive skills a student is expected to develop when embarking on an academic trajectory. We allow students to indicate which skill(s) they would like to focus on each time they query a recommendation from our system.\n",
    "\n",
    "Besides students' preferences, we also need to categorize courses according to their ILOs. First, we trained a multi-layer perceptron (MLP) for multilabel classification combining back-propagation, gradient descent, and Bayesian optimization for hyper-parameter search. The MLP expects a text embedding produced by the sentence transformer `all-MiniLM-L6-v2`. It is possible to use any sentence transformer, but that would require re-training the MLP. To classify an ILO, we encode it using the sentence transformer and then pass it through the MLP. For each ILO, we then obtain a vector with six values, each corresponding to one of the six cognitive skills. These values represent the independent probability that the ILO corresponds to that cognitive skill.\n",
    "\n",
    "Given the time it takes to compute these probabilities for all courses, we pre-compute them before setting up the recommender. Currently, these additional values are stored within `catalog.json`; found under `rec_sys_uni\\datasets\\data\\planners\\`. Each dictionary holds information about a course. To find the probabilities associated with each ILO of a specific course, look under the keyword `\"proba\"`.\n",
    "\n",
    "To collapse the probabilities for each ILO into a single set of six probabilities per course, we employ the [binomial distribution](https://en.wikipedia.org/wiki/Binomial_distribution). So, for each course and for each cognitive skill, we compute the collapsed probability by evaluating the following formula:\n",
    "<center>\n",
    "\n",
    "# $\\( P\\left(\\sum_{i}p_{s,i} \\geq 1\\right) = p_s \\)$\n",
    "</center>\n",
    "\n",
    "To learn more about how we trained the MLP, please check our notebook on [auto-labeling ILOs](Autolabeling_ILOs.ipynb). To learn more about how we collapsed the probabilities for each ILO, check our notebook on [aggregating ILO probabilities](Aggregating_ILO_Probs.ipynb).\n",
    "\n",
    "**Note:** We don't recommend running the notebooks, because many things have changed since they were implemented, and not all files and directories may be in the places each notebook expects. However, the processes and methods used remain the same.\n",
    "\n",
    "**Note:** The following information is deprecated. Our implementation of the Bloom-based recommender fell behind and should be upgraded to expect student inputs and course data using the new format.\n",
    "\n",
    "The `BloomBased` class in the recommendation system is designed to generate course recommendations for students based on Bloom's Taxonomy. This taxonomy categorizes cognitive skills into six areas: create, understand, apply, analyze, evaluate, and remember. The system allows students to express their preference for specific cognitive skills, and courses are recommended accordingly.\n",
    "\n",
    "#### Class Initialization\n",
    "`BloomBased.__init__(self, precomputed_blooms: bool = True, top_n: int | None = 4, score_alg: str = 'sum', scaler: str = 'None')`\n",
    "Initializes the `BloomBasedRecSys` with specified parameters.\n",
    "\n",
    "##### Parameters\n",
    "- `precomputed_blooms` (bool, optional): Determines if precomputed Bloom's values are used. Default is `True`. Currently, setting this to `False` is not implemented.\n",
    "- `top_n` (int, optional): Limits the number of recommendations, applying weights according to Bloom's taxonomy. If `None`, all courses are considered.\n",
    "- `score_alg` (str): Specifies the algorithm used for scoring. Current implementation supports 'sum'.\n",
    "- `scaler` (str): Specifies the scaling technique for the scores. Options are 'MaxMin' or 'None'. Current default is 'None'.\n",
    "\n",
    "#### Method: `recommend`\n",
    "##### `BloomBased.recommend(self, course_data, student_blooms)`\n",
    "Generates course recommendations based on student input and course data.\n",
    "\n",
    "##### Parameters\n",
    "- `course_data`: A list of dictionaries containing data about each course. The format is expected to follow that of `catalog.json`\n",
    "- `student_blooms`: Bloom's taxonomy preferences indicated by the student. A dictionary with a key and value for each cognitive skill is expected.\n",
    "\n",
    "##### Returns\n",
    "- `blooms_output`: The same course data with updated scores.\n",
    "\n",
    "##### Raises\n",
    "- `AssertionError`: If `precomputed_blooms` is set to `False`.\n",
    "\n",
    "**Notes**\n",
    "- The current implementation requires `precomputed_blooms` to be `True`. Setting it to `False` will raise an error.\n",
    "- The method reads precomputed Bloom's taxonomy values from a JSON file (`precomputed_blooms.json`) and uses them to match courses with student preferences.\n",
    "- The `recommend` method does not currently implement any scoring or filtering based on `top_n` or `scaler` parameters, but they are included for potential future extensions.\n",
    "\n",
    "**Deprecated Information**\n",
    "- The current implementation of the Bloom-based recommender is outdated and does not support recent changes in student input and course data formats. An upgrade is recommended to align with new system requirements. The bloom's values you can find here: `rec_sys_uni/datasets/data/course/precomputed_blooms.json`\n",
    "\n",
    "#### Future Improvements of Bloom's taxonomy:\n",
    "- Deploy and test the [DistilBERT](https://github.com/RyanLauQF/BloomBERT) or [DistilBERT EDM2022](https://github.com/SteveLEEEEE/EDM2022CLO) model for bloom-based recommender. This model should show better accuracy on the Bloom's taxonomy classification task, since they obtained high accuracy on their dataset, it would be interesting test on our data as well.\n",
    "- Add explanation of the bloom-based recommender based on shapley values/gini importance/LIME.\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/Bloom_importance.png\" alt=\"Bloom Explanation\" title=\"Bloom Explanation\"/>\n",
    "</p>\n",
    "\n",
    "Another type of explainability techniques perturbs the input to find which regions from it change the prediction to a larger degree. One such method is [LIME](https://arxiv.org/abs/1602.04938 ), which build a linear local approximator for each instance. It perturbs the input and tries to predict how the output is being changed with each local perturbation. The weights of the linear model for each token are used as saliency scores.\n",
    "\n",
    "See this [book](https://christophm.github.io/interpretable-ml-book/lime.html) on interpretability for more information on LIME and it's usage in NLP.\n",
    "\n",
    "LIME can also be used to explain which words contribute most to the polarity in sentiment analysis. See for instance this LIME visualization used to examine the words used for the classification in negation handling. It can be observed that DON'T is notreconized as DON NOT and is this not contributing to the negative polarity of this sentence, hence the positive classification instead of the (correct) negative one.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/LIME.png\" alt=\"LIME\" title=\"LIME\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "General structure of Bloom's Taxonomy model:\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/bloom.png\" alt=\"Bloom\" title=\"Bloom\"/>\n",
    "</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc198f1ab3f63c3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T16:05:43.059670600Z",
     "start_time": "2023-12-24T16:05:43.046671100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'remember': 3.3306690738754696e-16,\n",
       " 'understand': 1.0,\n",
       " 'apply': 5.436325878349635e-08,\n",
       " 'analyze': 2.886579864025407e-15,\n",
       " 'evaluate': 1.3245626817592893e-11,\n",
       " 'create': 8.374181348358434e-10}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rec_sys_uni.rec_systems.bloom_based_sys.bloom_based import BloomBased\n",
    "bloom_based = BloomBased(score_alg='sum',\n",
    "                         scaler='None')\n",
    "\n",
    "bloom_output = bloom_based.recommend(student_node.course_data,\n",
    "                                          student_node.student_input['keywords'])\n",
    "bloom_output['COR1002']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e3dc7189609d6f",
   "metadata": {},
   "source": [
    "## Explanation with Generative LLM:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36018857aceadc00",
   "metadata": {},
   "source": [
    "Currently, we are utilizing OpenAI's model to provide explanations for keywords and to summarize course descriptions. This is achieved through a combination of a prompt template and few-shot learning examples, as implemented in our `rec_sys_uni/rec_systems/llm_explanation/LLM.py` script. One significant advantage of using OpenAI is its capability to generate outputs in JSON format, which facilitates easier integration and manipulation of data in various applications.\n",
    "\n",
    "However, we are contemplating the development and deployment of our proprietary model for generating explanations. The primary motivation behind this shift is the potential for greater customization and accuracy. By fine-tuning our model specifically to our dataset, we aim to achieve performance that surpasses what we currently obtain with OpenAI. Own model will allow for more tailored and precise explanations, catering specifically to the unique characteristics and requirements of our data.\n",
    "\n",
    "Furthermore, developing our model could offer additional benefits such as enhanced control over the model's behavior, the ability to continuously improve and adapt the model based on incoming data and feedback, and potentially lower long-term costs associated with using a third-party service. While the initial development and training might require significant resources, the long-term benefits could be substantial, especially in terms of offering a more personalized and relevant user experience.\n",
    "\n",
    "#### Future Improvements of Explanation with Generative LLM:\n",
    "- Develop and deploy our model for generating explanations.\n",
    "- Split tasks into separate promts for better control over the output to adjust into the JSON format.\n",
    "- Deploy the knowledge graph to reduce the number of hallucinations and provide more robust explanations; [Graph Retrieval Augmented Generation](https://medium.com/@nebulagraph/graph-rag-the-new-llm-stack-with-knowledge-graphs-e1e902c504ed).\n",
    "- Add explanation of generated output based on retrieval from the knowledge graph.\n",
    "\n",
    "General structure of the explanation:\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/lmm.png\" alt=\"Explanation\" title=\"Explanation\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48f7136335bd12d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T15:58:27.244866700Z",
     "start_time": "2023-12-12T15:58:16.543312800Z"
    }
   },
   "outputs": [],
   "source": [
    "# To get the explanation for the course, we need to provide the student id and course code.\n",
    "explanation = rs.generate_explanation(student_node.id, \"SCI2039\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e67b790db9e6ad9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T15:58:29.759187100Z",
     "start_time": "2023-12-12T15:58:29.605035400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['course_code', 'course_title', 'summary_description', 'keywords_explanation'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanation.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe4d50ae6fd11be",
   "metadata": {},
   "source": [
    "# Example of Course Overview\n",
    "\n",
    "## SCI2039: Computer Science\n",
    "\n",
    "### Summary Description\n",
    "Computer Science is a comprehensive course that provides an overview of the discipline, covering a wide range of topics including algorithmic foundations of informatics, hardware issues, such as number systems and computer architectures, and software issues, such as operating systems, programming languages, compilers, networks, the Internet, and artificial intelligence. The course also includes practical lab sessions to investigate the concepts introduced. By the end of the course, students are expected to have developed experience in applying techniques from informatics, computer science, and programming for their own research and educational purposes.\n",
    "\n",
    "### Relevance to Other Disciplines\n",
    "\n",
    "- **Physics**\n",
    "  - While there is a moderate similarity between physics and the course description (keyword similarity: 0.9), the course does not directly cover physics topics. However, the analytical and problem-solving skills developed in physics can be beneficial in computer science.\n",
    "\n",
    "- **Maths**\n",
    "  - Mathematics is highly relevant to this course (keyword similarity: 0.67), as it forms the foundation of many computer science concepts, such as algorithms, logic, and discrete mathematics.\n",
    "\n",
    "- **Statistics**\n",
    "  - Statistics has some relevance to the course (keyword similarity: 0.28), particularly in the context of data analysis and interpretation, which are essential in computer science.\n",
    "\n",
    "- **AI**\n",
    "  - Artificial intelligence is directly relevant to the course (keyword similarity: 0.51), as it is explicitly mentioned in the course description. The course covers artificial intelligence as one of the key topics.\n",
    "\n",
    "- **Computer Science**\n",
    "  - The course is directly aligned with computer science (keyword similarity: 1.0), as it is the main subject of the course title. The course covers a broad range of computer science topics, providing a comprehensive introduction to the field.\n",
    "\n",
    "- **Chem**\n",
    "  - While there is a moderate similarity between chemistry and the course description (keyword similarity: 0.57), the course does not directly cover chemistry topics. However, the analytical and logical thinking skills developed in chemistry can be beneficial in computer science.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdc91686139a981",
   "metadata": {},
   "source": [
    "## Planner (Integer Linear Programming):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607c000075afe2cb",
   "metadata": {},
   "source": [
    "Our system has access to all courses' descriptions, titles, and ILOs. It combines all that information with user preferences to generate a preference score for each course. By incorporating course prerequisites and rules and regulations described in [Education and Examination Regulations (EER)](eer_2023-2024-mslas.pdf) into an expert system, we can go further than just ranking courses. Instead, we can generate academic trajectories that align with students' preferences. That is the role of our planner.\n",
    "\n",
    "Our planner is powered by an integer linear programming (ILP) solver bundled with [scipy](https://scipy.org/). We formulated the problem of generating an academic trajectory as a constrained scheduling problem. The system tries to allocate courses such that the total preference score is maximized, but certain constraints are never broken.\n",
    "\n",
    "At the time of writing this, our planner employs nearly 50000 constraints. Some of them come from EER. We went through the entire document and manually formulated each of the functional constraints according to the specifications. This accounts for less than half of the total number of constraints employed by the planner. Most of the constraints are the result of including course prerequisites.\n",
    "\n",
    "Course prerequisites are expressed in natural language. To get our planner to use them, they had to be translated into functional constraints. So, we translated each course prerequisite into propositional logic. Then, we developed a specialized parser to translate them from this intermediate form into functional constraints.\n",
    "\n",
    "**Note:** Currently, our parser takes the propositional formulas and writes them in conjunctive normal form (CNF) first. Then, it goes over each disjunctive clause and converts it to a functional constraint. While this process is easier to implement, it results in an explosion of functional constraints. A more efficient approach would be to write the propositional formulas in disjunctive normal form (DNF) and then convert them to functional constraints. This process would require including auxiliary variables, as specified in [IMMS Modeling Guide - Integer Programming Tricks](https://download.aimms.com/aimms/download/manuals/AIMMS3OM_IntegerProgrammingTricks.pdf).\n",
    "\n",
    "#### Class Initialization\n",
    "##### `UCMPlanner.__init__(self, reclib: Sequence[dict] | str) -> None`\n",
    "Initializes the `UCMPlanner` with the provided recommendation library (reclib).\n",
    "\n",
    "##### Parameters\n",
    "- `reclib` (Sequence[dict] | str): The recommendation library, which can be either a sequence of dictionaries representing courses or a string path to a JSON file containing this data.\n",
    "\n",
    "#### Internal Workings\n",
    "- Upon initialization, the planner processes course data, normalizes period information, and establishes various mappings and constraints crucial for the planning algorithm.\n",
    "- The planner formulates the academic trajectory as a constrained scheduling problem, aiming to maximize the total preference score while adhering to numerous constraints, including EER and course prerequisites.\n",
    "- Course prerequisites, initially in natural language, are converted into propositional logic and subsequently into functional constraints for the ILP solver.\n",
    "\n",
    "#### Methods\n",
    "##### `_update_ranks(self, courses: Sequence[dict])`\n",
    "Updates course rankings based on provided scores.\n",
    "\n",
    "##### Parameters\n",
    "- `courses` (Sequence[dict]): A sequence of dictionaries, each containing a course code and a score.\n",
    "\n",
    "##### `plan(self, courses: Sequence[dict])`\n",
    "Generates an academic trajectory based on the updated course scores.\n",
    "\n",
    "##### Parameters\n",
    "- `courses` (Sequence[dict]): A sequence of dictionaries containing course information and up-to-date scores.\n",
    "\n",
    "##### Returns\n",
    "- A structured academic plan (`pretty_tree`), organizing courses into periods and years, or `None` if no feasible plan is found.\n",
    "\n",
    "**Notes**\n",
    "- The planning process involves the generation of linear constraints and the application of a Mixed Integer Linear Programming (MILP) solver from `scipy`.\n",
    "- Additional constraints related to general education and concentration requirements (`gedu_ineq` and `conc_ineq`) are incorporated into the planning process as needed.\n",
    "- Typically, an entire academic trajectory can be generated in under four seconds. But sometimes the process may take longer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddd9a3a17ff5a09",
   "metadata": {},
   "source": [
    "## Warning System:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b92b198a50f26e",
   "metadata": {},
   "source": [
    "At present, our warning system is facing challenges in achieving accurate predictions due to imbalanced student data. This imbalance adversely affects the system's ability to correctly predict instances of academic failure. To address this issue, a more refined approach is being considered. We propose to define common features across courses, focusing on key skills such as physics, mathematics, law, etc. By identifying and utilizing these skill-based features, we aim to improve the prediction accuracy of potential academic failures. Furthermore, this approach will enable us to provide students with tailored recommendations on which courses to undertake in order to develop the relevant skills needed to succeed in courses they are predicted to struggle with.\n",
    "\n",
    "Previously, we experimented with using embeddings, assigning weights based on students' grades. However, this method resulted in overly generalized embeddings, leading to indistinct differentiation between them. \n",
    "\n",
    "Currently, we are now employing a Random Forest Classifier to predict academic failures. This classifier offers the advantage of feature importance analysis, which we utilize to determine the key aspects contributing to potential failures or passing. Additionally, we leverage the capabilities of a Sentence Transformer to check these potential courses and give warning recommendations with the matching score with the predicted course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d9f90c7daa2f63",
   "metadata": {},
   "source": [
    "Warning model implementation and usage: you can find the warning model in the `rec_sys_uni/rec_systems/warning_model/warning_model.py` script and `notebooks/Warning_Model.ipynb` notebook. \n",
    "\n",
    "General structure of the warning model:\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/wp.png\" alt=\"Warning\" title=\"Warning\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28da1aa787914172",
   "metadata": {},
   "source": [
    "### Collaborative Filtering as a Warning System:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdce2ed5dda1e24",
   "metadata": {},
   "source": [
    "We implemented a matrix factorization method to predict the course grade, utilizing student data that includes academic grades. However, this system is not yet fully integrated, as it requires access to real student IDs from Maastricht University. Additionally, the model necessitates periodic updates to maintain its relevance and accuracy.\n",
    "\n",
    "A significant challenge we face with this approach is the limited number of courses taken by individual students. This situation leads to a sparse matrix, which complicates the course grade prediction. Sparse data can significantly hinder the effectiveness of matrix factorization techniques, as these methods rely on a denser dataset to make accurate predictions and recommendations. As an additional improvement, we can average results of matrix factorization and Restricted Boltzmann Machine (RBM) to get better results, since this combination showed outperforming results with the [sparse Netflix dataset](https://netflixtechblog.com/netflix-recommendations-beyond-the-5-stars-part-1-55838468f429) or Non-Matrix Factorization along with Supper Vector Machine. However, we do not have so much data and features of courses. Therefore, we need to collect course features to get better results.\n",
    "\n",
    "To address this issue, we are exploring alternative methods. One promising approach is the use of a Knowledge Graph. This method can effectively leverage the available student data to provide course recommendations. A Knowledge Graph, by its very nature, is adept at handling sparse data. It creates a network of entities (such as students, courses, skills) and their interrelations, allowing for a more nuanced understanding of student profiles and course attributes. This enhanced understanding can lead to more accurate and personalized course recommendations, even when dealing with limited student course histories.\n",
    "\n",
    "By incorporating a Knowledge Graph, we aim to overcome the limitations posed by sparse data and improve the overall effectiveness of our recommendation system. This approach not only promises to deliver better-tailored course suggestions to students but also has the potential to evolve and scale as more data becomes available or as the educational offerings at Maastricht University expand and change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21687e5daee8f24",
   "metadata": {},
   "source": [
    "Collaborative Filtering implementation: you can find the collaborative filtering model in the `notebooks/Collaborative_Filtering.ipynb` notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14281f3542d46806",
   "metadata": {},
   "source": [
    "## Student Model:\n",
    "\n",
    "General structure of the student model:\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/sm.png\" alt=\"Student\" title=\"Student\"/>\n",
    "</p>\n",
    "\n",
    "### Content-based:\n",
    "\n",
    "Currently, we utilize the Sentence Transformer [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5), which has shown the best performance on the [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard). The purpose of this model is to compare the similarities of courses taken by students with the course catalog that are similar to those already taken. However, this approach lacks transparency and explainability, since it is not clear why the recommended courses are similar to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19046f52c9baf185",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T17:39:48.215451Z",
     "start_time": "2023-12-24T17:39:48.010697800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SSC1005': 0.6391,\n",
       " 'SSC1025': 0.5895,\n",
       " 'SSC2019': 0.6427,\n",
       " 'SSC2062': 0.6381,\n",
       " 'SSC1007': 0.5868,\n",
       " 'SSC2004': 0.5462}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rec_sys_uni.rec_systems.content_based_sys.content_based import ContentBased\n",
    "from rec_sys_uni.datasets.datasets import get_student_data\n",
    "content_based = ContentBased(\n",
    "    model_name='BAAI/bge-large-en-v1.5',\n",
    "    distance='cos',\n",
    "    score_alg='sum',\n",
    "    scaler='None')\n",
    "\n",
    "student_data = get_student_data()\n",
    "\n",
    "content_based_output = content_based.compute_course_similarity(\n",
    "                                                                student_node.results[\"recommended_courses\"].keys(),\n",
    "                                                                student_data\n",
    "                                                                )\n",
    "content_based_output['COR1002']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dff806b443d18394",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T17:16:23.932003900Z",
     "start_time": "2023-12-24T17:16:23.772972600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'courses_taken': {'SSC1005': {'passed': True,\n",
       "   'grade': 7.1,\n",
       "   'year': 2022,\n",
       "   'period': 1},\n",
       "  'COR1003': {'passed': True, 'grade': 6.5, 'year': 2022, 'period': 1},\n",
       "  'SSC1025': {'passed': True, 'grade': 7.6, 'year': 2022, 'period': 2},\n",
       "  'SSC2019': {'passed': True, 'grade': 6.0, 'year': 2022, 'period': 2},\n",
       "  'SSC2062': {'passed': True, 'grade': 6.4, 'year': 2023, 'period': 5},\n",
       "  'SSC1007': {'passed': True, 'grade': 7.2, 'year': 2023, 'period': 4},\n",
       "  'COR1004': {'passed': False, 'grade': 5.7, 'year': 2023, 'period': 5},\n",
       "  'SSC2004': {'passed': True, 'grade': 6.9, 'year': 2023, 'period': 5}}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de99d7e609962b0f",
   "metadata": {},
   "source": [
    "### Statistical-Student-Based: (Improvements of Recommendation System)\n",
    "\n",
    "**Hidden Markov Models** (HMMs) largely used to assign the correct label sequence to sequential data or assess the probability of a given label and data sequence. These models are finite state machines characterised by a number of states, transitions between these states, and output symbols emitted while in each state. The HMM is an extension to the Markov chain, where each state corresponds deterministically to a given event. In the HMM the observation is a probabilistic function of the state. HMMs share the Markov chain's assumption, being that the probability of transition from one state to another only depends on the current state - i.e. the series of states that led to the current state are not used. They are also time invariant.\n",
    "\n",
    "The HMM is a directed graph, with probability weighted edges (representing the probability of a transition between the source and sink states) where each vertex emits an output symbol when entered. The symbol (or observation) is non-deterministically generated. For this reason, knowing that a sequence of output observations was generated by a given HMM does not mean that the corresponding sequence of states (and what the current state is) is known. This is the 'hidden' in the hidden markov model.\n",
    "\n",
    "Formally, a HMM can be characterised by:\n",
    "\n",
    "- the output observation alphabet. This is the set of symbols which may be\n",
    "  observed as output of the system.\n",
    "- the set of states.\n",
    "- the transition probabilities $a_{ij} = P(s_t = j | s_{t-1} = i)$. These\n",
    "  represent the probability of transition to each state from a given state.\n",
    "- the output probability matrix $b_i(k) = P(X_t = o_k | s_t = i)$. These\n",
    "  represent the probability of observing each symbol in a given state.\n",
    "- the initial state distribution. This gives the probability of starting\n",
    "  in each state.\n",
    "\n",
    "**HMM** can give probabilities of next courses based on the previous student data. However, HMMs have joint probabilities, which can fail on one course sequence, which leads to sequence errors of next predicted course. Therefore, we can try to use a more advanced model, such as [Conditional Random Fields](https://en.wikipedia.org/wiki/Conditional_random_field) (CRFs), which can give conditional probabilities of next courses based on the previous student data. CRFs are a class of statistical modelling method often applied in pattern recognition and machine learning and used for structured prediction. Whereas an ordinary classifier predicts courses for a single sample without regard to \"neighboring\" samples, a CRF can take context into account: The prediction of courses for one course may depend on other courses.\n",
    "\n",
    "Although our feature function in both models can be course labels, it is better to consider more advanced feature extraction methods to generalize sequence labels. This approach determines which courses should be aligns with course features instead of course labels."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Course Reranker:\n",
    "\n",
    "Since we receive different recommendations from our models, we need to apply a re-ranking strategy to determine the final recommendation. This can be achieved by using reciprocal rank fusion.\n",
    "<center>\n",
    "\n",
    "# $\\[ \\sum_{d \\in D} \\frac{1}{k + r(d)} \\]$\n",
    "\n",
    "</center>\n",
    "\n",
    "- `k`: is constant value, which is used to assign weights to the models.\n",
    "- `d`: is model.\n",
    "- `r(d)`: recommendation of the model which sorted by certain criteria.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c6ae47cdb47fd2f"
  },
  {
   "cell_type": "markdown",
   "id": "9e0ca8f476123ea4",
   "metadata": {},
   "source": [
    "## Knowledge Graph:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447d9280370945c6",
   "metadata": {},
   "source": [
    "Knowledge Graph is a hot topic in fields of representation learning, mining techniques and recommender systems. \n",
    "\n",
    "Relevant papers:\n",
    "- [Recommender systems based on graph embedding techniques: A comprehensive review](https://arxiv.org/pdf/2109.09587.pdf)\n",
    "- [ColdGuess: A General and Effective Relational Graph Convolutional Network to Tackle Cold Start Cases](https://www.mlgworkshop.org/2022/papers/MLG22_paper_0866.pdf)\n",
    "- [Topological Representation Learning for E-commerce Shopping Behaviors](https://www.mlgworkshop.org/2023/papers/MLG__KDD_2023_paper_13.pdf)\n",
    "\n",
    "A Knowledge Graph can address problems with the explainability of models and provide more accurate results by introducing a structured knowledge base of courses and students. We attempt to create a knowledge graph of courses using current data by extracting relevant keywords from course descriptions and Intended Learning Outcomes (ILOs). These extracted keywords are then compared with other course descriptions and accepted as keywords if they meet a high similarity threshold. This approach demonstrates which courses and keywords share similar concepts.\n",
    "\n",
    "\n",
    "General Knowledge graph explanation:\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/kg.png\" alt=\"Knowledge Graph\" title=\"Knowledge Graph\"/>\n",
    "</p>\n",
    "\n",
    "All process of creating this knowledge graph can be found in the `notebooks/Knowledge_Graph.ipynb` notebook along with course clustering, LDAs, keywords extraction, and other techniques.\n",
    "\n",
    "Knowledge graph `knowledge_graph/knowledge_graph.html` itself.\n",
    "\n",
    "Course clustering `knowledge_graph/course_clustering.html` representation:\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/course_clustering.png\" alt=\"Course Clustering\" title=\"Course Clustering\"/>\n",
    "</p>\n",
    "\n",
    "Latent Dirichlet Allocation (LDA) `knowledge_graph/lda.html` representation:\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/lda.png\" alt=\"LDA\" title=\"LDA\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4d5ea652b09d4b",
   "metadata": {},
   "source": [
    "Moreover, we extract the semantic role labeling of all course descriptions, as they can be utilized to discern the primary concepts of the course. Semantic Role Labeling (SRL) is the task of assigning labels to words in a sentence to identify their role within that sentence. SRL, a task in natural language processing, is also referred to as shallow semantic parsing. The semantic roles, often termed as arguments, are designated based on both the verb and its context. Consequently, verbs can determine the Bloom's taxonomy level, and the context of verbs can specify the subject to which this Bloom level is applied.. This approach can be used for Bloom's taxonomy explanation.\n",
    "\n",
    "[Open Syllabus Galaxy](https://galaxy.opensyllabus.org/) is dataset of syllabi from universities, articles, books around the world. It contains 6.1 million syllabi from 80 countries. The dataset can be accessed through request to them. This dataset is invaluable for creating a more comprehensive knowledge graph and utilizing its labels to define course features, aiding in Warning Prediction, Student Modeling, and training our Generative Large Language Model to provide explanations with references. Furthermore, there is ongoing work to create Intended Learning Outcomes (ILOs) for courses based on their descriptions and materials, which could be beneficial for articulating relevant ILOs.\n",
    "\n",
    "By creating a CRFs (Conditional Random Fields) model based on student data, you can generate a student knowledge graph with probability transitions along with course features, which can be derived from the Open Syllabus dataset.\n",
    "\n",
    "Additionally, European Comission released the [paper](https://esco.ec.europa.eu/en/about-esco/data-science-and-esco/leveraging-artificial-intelligence-maintain-esco-occupations-pillar) on the alignment of citizen occupations and skills taxonomy, which can be useful in our current work of creating a knowledge graph for the Recommender System.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cc4e586db00a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
